{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f064925",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03595d",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe0d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Buffer distance: 200m\n",
      "âœ“ Bike mode filter: velo\n",
      "âœ“ Silver data: data/silver\n",
      "âœ“ Gold output: data/gold\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Windows compatibility fix for PySpark\n",
    "if sys.platform == \"win32\":\n",
    "    import socketserver\n",
    "    if not hasattr(socketserver, 'UnixStreamServer'):\n",
    "        socketserver.UnixStreamServer = socketserver.TCPServer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, countDistinct, desc, lit, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Load configuration\n",
    "with open(\"../config/config.yml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract key parameters\n",
    "buffer_m = config[\"params\"][\"buffer_m\"]\n",
    "bike_mode = config[\"filters\"][\"bike_mode_value\"]\n",
    "silver_dir = config[\"paths\"][\"silver_dir\"]\n",
    "gold_dir = config[\"paths\"][\"gold_dir\"]\n",
    "\n",
    "print(f\"âœ“ Buffer distance: {buffer_m}m\")\n",
    "print(f\"âœ“ Bike mode filter: {bike_mode}\")\n",
    "print(f\"âœ“ Silver data: {silver_dir}\")\n",
    "print(f\"âœ“ Gold output: {gold_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "093dc224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Spark version: 3.5.3\n",
      "âœ“ Python: c:\\Users\\medma\\Documents\\2025-2026\\Big Data\\datathon_velomenaj\\venv\\Scripts\\python.exe\n",
      "âœ“ HADOOP_HOME: C:\\hadoop\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session (Windows configuration)\n",
    "import tempfile\n",
    "\n",
    "# Set Python executable for Spark workers\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Local temp directory\n",
    "local_temp = tempfile.gettempdir()\n",
    "os.environ['SPARK_LOCAL_DIRS'] = local_temp\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module2_SpatialUsage\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .config(\"spark.ui.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.local.dir\", local_temp) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"âœ“ Spark version: {spark.version}\")\n",
    "print(f\"âœ“ Python: {sys.executable}\")\n",
    "print(f\"âœ“ HADOOP_HOME: {os.environ.get('HADOOP_HOME', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616b1c1",
   "metadata": {},
   "source": [
    "## 2. Load Silver Data (CSV)\n",
    "\n",
    "Les donnÃ©es Silver sont gÃ©nÃ©rÃ©es par le notebook pandas (`02_spatial_usage_pandas.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c265537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded silver_amenagements: 3 rows\n",
      "âœ“ Loaded silver_sites: 3 rows\n",
      "âœ“ Loaded silver_channels: 5 rows\n",
      "âœ“ Loaded silver_measures: 120 rows\n"
     ]
    }
   ],
   "source": [
    "# Load Silver CSVs\n",
    "df_amenagements = spark.read.csv(\n",
    "    f\"../{silver_dir}/silver_amenagements.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_sites = spark.read.csv(\n",
    "    f\"../{silver_dir}/silver_sites.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_channels = spark.read.csv(\n",
    "    f\"../{silver_dir}/silver_channels.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_measures = spark.read.csv(\n",
    "    f\"../{silver_dir}/silver_measures.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded silver_amenagements: {df_amenagements.count()} rows\")\n",
    "print(f\"âœ“ Loaded silver_sites: {df_sites.count()} rows\")\n",
    "print(f\"âœ“ Loaded silver_channels: {df_channels.count()} rows\")\n",
    "print(f\"âœ“ Loaded silver_measures: {df_measures.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2412dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Amenagements ===\n",
      "+--------------+---------------+----------------+-------------+----------+--------------------------------------+------------+------------+------------+\n",
      "|amenagement_id|annee_livraison|type_amenagement|environnement|longueur_m|geom_wkt                              |centroid_lat|centroid_lon|commune     |\n",
      "+--------------+---------------+----------------+-------------+----------+--------------------------------------+------------+------------+------------+\n",
      "|AMEN_001      |2020           |Piste cyclable  |Urbain       |500.0     |LINESTRING(4.835 45.764, 4.836 45.765)|45.764      |4.835       |Lyon        |\n",
      "|AMEN_002      |2021           |Bande cyclable  |PÃ©riurbain   |300.0     |LINESTRING(4.840 45.770, 4.841 45.771)|45.77       |4.84        |Villeurbanne|\n",
      "|AMEN_003      |2019           |Voie verte      |Urbain       |800.0     |LINESTRING(4.850 45.750, 4.851 45.751)|45.75       |4.85        |Lyon        |\n",
      "+--------------+---------------+----------------+-------------+----------+--------------------------------------+------------+------------+------------+\n",
      "\n",
      "=== Sites ===\n",
      "+--------+------+-----+------------+\n",
      "| site_id|   lat|  lon|     commune|\n",
      "+--------+------+-----+------------+\n",
      "|SITE_001|45.764|4.835|        Lyon|\n",
      "|SITE_002| 45.77| 4.84|Villeurbanne|\n",
      "|SITE_003| 45.78| 4.86|Villeurbanne|\n",
      "+--------+------+-----+------------+\n",
      "\n",
      "=== Channels ===\n",
      "+----------+--------+-------+-----+\n",
      "|channel_id| site_id|   mode| sens|\n",
      "+----------+--------+-------+-----+\n",
      "|  CHAN_001|SITE_001|   velo| Nord|\n",
      "|  CHAN_002|SITE_001|   velo|  Sud|\n",
      "|  CHAN_003|SITE_002|   velo|  Est|\n",
      "|  CHAN_004|SITE_003|   velo|Ouest|\n",
      "|  CHAN_005|SITE_002|voiture| Nord|\n",
      "+----------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview data\n",
    "print(\"=== Amenagements ===\")\n",
    "df_amenagements.show(truncate=False)\n",
    "\n",
    "print(\"=== Sites ===\")\n",
    "df_sites.show()\n",
    "\n",
    "print(\"=== Channels ===\")\n",
    "df_channels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b46ebe",
   "metadata": {},
   "source": [
    "## 3. Spatial Join: Link Counters to Infrastructure\n",
    "\n",
    "Utilisation de la formule Haversine pour calculer les distances et trouver les compteurs proches (â‰¤ 200m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee82c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Haversine function using native Spark SQL (no Python UDF)\n"
     ]
    }
   ],
   "source": [
    "# Haversine distance using NATIVE Spark SQL functions (no Python UDF = no worker crash!)\n",
    "from pyspark.sql.functions import radians, sin, cos, sqrt, atan2, lit\n",
    "\n",
    "def haversine_spark(lat1_col, lon1_col, lat2_col, lon2_col):\n",
    "    \"\"\"\n",
    "    Calculate distance in meters between two points using Spark SQL native functions.\n",
    "    This avoids Python UDF issues on Windows.\n",
    "    \"\"\"\n",
    "    R = 6371000  # Earth radius in meters\n",
    "    \n",
    "    # Convert to radians\n",
    "    lat1_rad = radians(lat1_col)\n",
    "    lon1_rad = radians(lon1_col)\n",
    "    lat2_rad = radians(lat2_col)\n",
    "    lon2_rad = radians(lon2_col)\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return lit(R) * c\n",
    "\n",
    "print(\"âœ“ Haversine function using native Spark SQL (no Python UDF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42addc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 2 amenagement-site pairs within 200m\n",
      "+--------------+--------+--------+--------+--------+--------+----------+\n",
      "|amenagement_id|amen_lat|amen_lon| site_id|site_lat|site_lon|distance_m|\n",
      "+--------------+--------+--------+--------+--------+--------+----------+\n",
      "|      AMEN_001|  45.764|   4.835|SITE_001|  45.764|   4.835|       0.0|\n",
      "|      AMEN_002|   45.77|    4.84|SITE_002|   45.77|    4.84|       0.0|\n",
      "+--------------+--------+--------+--------+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross join amenagements and sites\n",
    "df_cross = df_amenagements.select(\n",
    "    col(\"amenagement_id\"),\n",
    "    col(\"centroid_lat\").alias(\"amen_lat\"),\n",
    "    col(\"centroid_lon\").alias(\"amen_lon\")\n",
    ").crossJoin(\n",
    "    df_sites.select(\n",
    "        col(\"site_id\"),\n",
    "        col(\"lat\").alias(\"site_lat\"),\n",
    "        col(\"lon\").alias(\"site_lon\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate distance using native Spark SQL functions (no UDF!)\n",
    "df_within_buffer = df_cross.withColumn(\n",
    "    \"distance_m\",\n",
    "    haversine_spark(\n",
    "        col(\"amen_lat\"), col(\"amen_lon\"),\n",
    "        col(\"site_lat\"), col(\"site_lon\")\n",
    "    )\n",
    ").filter(col(\"distance_m\") <= buffer_m)\n",
    "\n",
    "print(f\"âœ“ Found {df_within_buffer.count()} amenagement-site pairs within {buffer_m}m\")\n",
    "df_within_buffer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5503fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 3 amenagement-channel links (bike mode only)\n",
      "+--------+--------------+--------+--------+--------+--------+----------+----------+----+----+\n",
      "| site_id|amenagement_id|amen_lat|amen_lon|site_lat|site_lon|distance_m|channel_id|mode|sens|\n",
      "+--------+--------------+--------+--------+--------+--------+----------+----------+----+----+\n",
      "|SITE_001|      AMEN_001|  45.764|   4.835|  45.764|   4.835|       0.0|  CHAN_002|velo| Sud|\n",
      "|SITE_001|      AMEN_001|  45.764|   4.835|  45.764|   4.835|       0.0|  CHAN_001|velo|Nord|\n",
      "|SITE_002|      AMEN_002|   45.77|    4.84|   45.77|    4.84|       0.0|  CHAN_003|velo| Est|\n",
      "+--------+--------------+--------+--------+--------+--------+----------+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join with channels and filter for bike mode only\n",
    "df_amen_channels = df_within_buffer.join(\n",
    "    df_channels,\n",
    "    on=\"site_id\",\n",
    "    how=\"inner\"\n",
    ").filter(col(\"mode\") == bike_mode)\n",
    "\n",
    "print(f\"âœ“ Found {df_amen_channels.count()} amenagement-channel links (bike mode only)\")\n",
    "df_amen_channels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f531ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created gold_link_amenagement_channel (3 rows)\n",
      "+--------------+----------+--------+----------+\n",
      "|amenagement_id|channel_id| site_id|distance_m|\n",
      "+--------------+----------+--------+----------+\n",
      "|      AMEN_001|  CHAN_001|SITE_001|       0.0|\n",
      "|      AMEN_002|  CHAN_003|SITE_002|       0.0|\n",
      "|      AMEN_001|  CHAN_002|SITE_001|       0.0|\n",
      "+--------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create gold_link_amenagement_channel\n",
    "gold_link = df_amen_channels.select(\n",
    "    \"amenagement_id\",\n",
    "    \"channel_id\",\n",
    "    \"site_id\",\n",
    "    \"distance_m\"\n",
    ").distinct()\n",
    "\n",
    "# Cache for reuse\n",
    "gold_link.cache()\n",
    "\n",
    "print(f\"âœ“ Created gold_link_amenagement_channel ({gold_link.count()} rows)\")\n",
    "gold_link.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc465d",
   "metadata": {},
   "source": [
    "## 4. Aggregate Daily Flows per Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee729ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Joined 90 measure records\n",
      "+--------------+----------+----------+----+\n",
      "|amenagement_id|channel_id|      date|flux|\n",
      "+--------------+----------+----------+----+\n",
      "|      AMEN_001|  CHAN_001|2023-06-01| 150|\n",
      "|      AMEN_001|  CHAN_002|2023-06-01| 100|\n",
      "|      AMEN_002|  CHAN_003|2023-06-01| 250|\n",
      "|      AMEN_001|  CHAN_001|2023-06-02| 152|\n",
      "|      AMEN_001|  CHAN_002|2023-06-02| 101|\n",
      "|      AMEN_002|  CHAN_003|2023-06-02| 253|\n",
      "|      AMEN_001|  CHAN_001|2023-06-03| 154|\n",
      "|      AMEN_001|  CHAN_002|2023-06-03| 102|\n",
      "|      AMEN_002|  CHAN_003|2023-06-03| 256|\n",
      "|      AMEN_001|  CHAN_001|2023-06-04| 156|\n",
      "+--------------+----------+----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join links with measures (only valid ones)\n",
    "df_flows = gold_link.join(\n",
    "    df_measures.filter(col(\"is_valid\") == True),\n",
    "    on=\"channel_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Joined {df_flows.count()} measure records\")\n",
    "df_flows.select(\"amenagement_id\", \"channel_id\", \"date\", \"flux\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc5b9256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created gold_flow_amenagement_daily (60 rows)\n",
      "+--------------+----------+-----------+----------+\n",
      "|amenagement_id|      date|flux_estime|n_channels|\n",
      "+--------------+----------+-----------+----------+\n",
      "|      AMEN_001|2023-06-01|        250|         2|\n",
      "|      AMEN_001|2023-06-02|        253|         2|\n",
      "|      AMEN_001|2023-06-03|        256|         2|\n",
      "|      AMEN_001|2023-06-04|        259|         2|\n",
      "|      AMEN_001|2023-06-05|        262|         2|\n",
      "|      AMEN_001|2023-06-06|        265|         2|\n",
      "|      AMEN_001|2023-06-07|        268|         2|\n",
      "|      AMEN_001|2023-06-08|        271|         2|\n",
      "|      AMEN_001|2023-06-09|        274|         2|\n",
      "|      AMEN_001|2023-06-10|        277|         2|\n",
      "|      AMEN_001|2023-06-11|        280|         2|\n",
      "|      AMEN_001|2023-06-12|        283|         2|\n",
      "|      AMEN_001|2023-06-13|        286|         2|\n",
      "|      AMEN_001|2023-06-14|        289|         2|\n",
      "|      AMEN_001|2023-06-15|        292|         2|\n",
      "|      AMEN_001|2023-06-16|        295|         2|\n",
      "|      AMEN_001|2023-06-17|        298|         2|\n",
      "|      AMEN_001|2023-06-18|        301|         2|\n",
      "|      AMEN_001|2023-06-19|        304|         2|\n",
      "|      AMEN_001|2023-06-20|        307|         2|\n",
      "+--------------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate by amenagement_id and date\n",
    "gold_flow_daily = df_flows.groupBy(\"amenagement_id\", \"date\").agg(\n",
    "    sum(\"flux\").alias(\"flux_estime\"),\n",
    "    countDistinct(\"channel_id\").alias(\"n_channels\")\n",
    ").orderBy(\"amenagement_id\", \"date\")\n",
    "\n",
    "# Cache for reuse\n",
    "gold_flow_daily.cache()\n",
    "\n",
    "print(f\"âœ“ Created gold_flow_amenagement_daily ({gold_flow_daily.count()} rows)\")\n",
    "gold_flow_daily.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f88d4b",
   "metadata": {},
   "source": [
    "## 5. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a55fe413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ No duplicate links\n",
      "âœ“ 2 amenagements with flow data\n",
      "âœ“ All flux values are non-negative\n",
      "âœ“ All days have at least 1 channel\n",
      "\n",
      "ðŸŽ‰ All quality checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Check 1: No duplicate amenagement-channel links\n",
    "duplicate_links = gold_link.groupBy(\"amenagement_id\", \"channel_id\").count().filter(col(\"count\") > 1)\n",
    "assert duplicate_links.count() == 0, \"FAILED: Found duplicate amenagement-channel links\"\n",
    "print(\"âœ“ No duplicate links\")\n",
    "\n",
    "# Check 2: Count amenagements with data\n",
    "amen_with_data = gold_flow_daily.select(\"amenagement_id\").distinct().count()\n",
    "print(f\"âœ“ {amen_with_data} amenagements with flow data\")\n",
    "\n",
    "# Check 3: flux_estime should be non-negative\n",
    "negative_flux = gold_flow_daily.filter(col(\"flux_estime\") < 0)\n",
    "assert negative_flux.count() == 0, \"FAILED: Found negative flux values\"\n",
    "print(\"âœ“ All flux values are non-negative\")\n",
    "\n",
    "# Check 4: n_channels should be >= 1\n",
    "zero_channels = gold_flow_daily.filter(col(\"n_channels\") < 1)\n",
    "assert zero_channels.count() == 0, \"FAILED: Found days with 0 channels\"\n",
    "print(\"âœ“ All days have at least 1 channel\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All quality checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53182433",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36290a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary by infrastructure:\n",
      "+--------------+----------+----------+--------------+------------+\n",
      "|amenagement_id|total_days|total_flux|avg_daily_flux|max_channels|\n",
      "+--------------+----------+----------+--------------+------------+\n",
      "|      AMEN_001|        30|      8805|         293.5|           2|\n",
      "|      AMEN_002|        30|      8805|         293.5|           1|\n",
      "+--------------+----------+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary by amenagement\n",
    "from pyspark.sql.functions import avg, max as spark_max\n",
    "\n",
    "summary = gold_flow_daily.groupBy(\"amenagement_id\").agg(\n",
    "    count(\"date\").alias(\"total_days\"),\n",
    "    sum(\"flux_estime\").alias(\"total_flux\"),\n",
    "    avg(\"flux_estime\").alias(\"avg_daily_flux\"),\n",
    "    spark_max(\"n_channels\").alias(\"max_channels\")\n",
    ").orderBy(desc(\"avg_daily_flux\"))\n",
    "\n",
    "print(\"Summary by infrastructure:\")\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dcdb90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daily flows for AMEN_001:\n",
      "+--------------+----------+-----------+----------+\n",
      "|amenagement_id|      date|flux_estime|n_channels|\n",
      "+--------------+----------+-----------+----------+\n",
      "|      AMEN_001|2023-06-01|        250|         2|\n",
      "|      AMEN_001|2023-06-02|        253|         2|\n",
      "|      AMEN_001|2023-06-03|        256|         2|\n",
      "|      AMEN_001|2023-06-04|        259|         2|\n",
      "|      AMEN_001|2023-06-05|        262|         2|\n",
      "|      AMEN_001|2023-06-06|        265|         2|\n",
      "|      AMEN_001|2023-06-07|        268|         2|\n",
      "|      AMEN_001|2023-06-08|        271|         2|\n",
      "|      AMEN_001|2023-06-09|        274|         2|\n",
      "|      AMEN_001|2023-06-10|        277|         2|\n",
      "+--------------+----------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Daily flows for AMEN_001\n",
    "print(\"\\nDaily flows for AMEN_001:\")\n",
    "gold_flow_daily.filter(col(\"amenagement_id\") == \"AMEN_001\") \\\n",
    "    .orderBy(\"date\") \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a22571",
   "metadata": {},
   "source": [
    "## 7. Save Outputs\n",
    "\n",
    "Sauvegarde en CSV (format actuel). Option Parquet disponible pour la production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9b88d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved Gold outputs as CSV to data/gold/\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "os.makedirs(f\"../{gold_dir}\", exist_ok=True)\n",
    "\n",
    "# === OPTION 1: Save as CSV (current) ===\n",
    "gold_link.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\n",
    "    f\"../{gold_dir}/gold_link_amenagement_channel_spark\"\n",
    ")\n",
    "\n",
    "gold_flow_daily.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\n",
    "    f\"../{gold_dir}/gold_flow_amenagement_daily_spark\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Saved Gold outputs as CSV to {gold_dir}/\")\n",
    "\n",
    "# === OPTION 2: Save as Parquet (for production) ===\n",
    "# Uncomment when Hadoop is properly configured\n",
    "# gold_link.write.mode(\"overwrite\").parquet(f\"../{gold_dir}/gold_link_amenagement_channel\")\n",
    "# gold_flow_daily.write.mode(\"overwrite\").parquet(f\"../{gold_dir}/gold_flow_amenagement_daily\")\n",
    "# print(f\"âœ“ Saved Gold outputs as Parquet to {gold_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c1c56",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae5c8a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"âœ“ Spark session stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105118b7",
   "metadata": {},
   "source": [
    "## 9. Notes\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. **GÃ©nÃ©rer les donnÃ©es mock** â†’ ExÃ©cuter `02_spatial_usage_pandas.ipynb`\n",
    "2. **Traitement PySpark** â†’ ExÃ©cuter ce notebook\n",
    "3. **Scoring** â†’ Module 3 consomme les fichiers Gold\n",
    "\n",
    "### Fichiers produits\n",
    "\n",
    "```\n",
    "data/gold/\n",
    "â”œâ”€â”€ gold_link_amenagement_channel_spark/   (CSV)\n",
    "â””â”€â”€ gold_flow_amenagement_daily_spark/     (CSV)\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
