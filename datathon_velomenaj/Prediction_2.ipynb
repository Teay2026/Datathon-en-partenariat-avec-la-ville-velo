{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "init_spark",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/05 17:27:24 WARN Utils: Your hostname, Younesss-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.42.233.166 instead (on interface en0)\n",
      "26/01/05 17:27:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/05 17:27:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/05 17:27:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created Successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# IMPORT NATIVE ML FUNCTIONS TO AVOID UDFS\n",
    "try:\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "except ImportError:\n",
    "    print(\"WARNING: pyspark.ml.functions.vector_to_array not found. Using fallback.\")\n",
    "\n",
    "# =========================\n",
    "# 1) SPARK SESSION SETUP\n",
    "# =========================\n",
    "\n",
    "# FIX: Force python vars to match driver to avoid version mismatch (Driver vs Worker)\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable \n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Velomenaj_Prediction_TopTier\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark Session Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size (Global): 456 rows\n",
      "+-------------------------------------------------+---------------------+------------------+-----------------+--------+\n",
      "|amenagement_id                                   |nom                  |centroid_lat      |centroid_lon     |score   |\n",
      "+-------------------------------------------------+---------------------+------------------+-----------------+--------+\n",
      "|pvo_patrimoine_voirie.pvoamenagementcyclable.6026|Rue Léon Blum        |45.764272317499994|4.917622205000001|0.19752 |\n",
      "|pvo_patrimoine_voirie.pvoamenagementcyclable.2568|Pont d'Herbens       |45.795171905      |4.990913535000001|0.201999|\n",
      "|pvo_patrimoine_voirie.pvoamenagementcyclable.6738|Rue de Saint-Cyr     |45.786508309999995|4.808178546666666|0.330917|\n",
      "|pvo_patrimoine_voirie.pvoamenagementcyclable.2827|Quai Victor Augagneur|45.757608444      |4.840734573      |0.812162|\n",
      "|pvo_patrimoine_voirie.pvoamenagementcyclable.282 |Pont Lafayette       |45.763568305      |4.84062562       |0.819798|\n",
      "+-------------------------------------------------+---------------------+------------------+-----------------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) LOAD DATA (Features + Targets)\n",
    "# =========================\n",
    "\n",
    "# A. Load Features (Infrastructure)\n",
    "path_amenagements = \"file:\" + os.path.abspath(\"data_temp/silver_amenagements_with_coordinates\")\n",
    "df_raw_features = spark.read.parquet(path_amenagements)\n",
    "# FIX: Add prefix to match output format of Scoring2\n",
    "df_raw_features = df_raw_features.withColumn(\n",
    "    \"amenagement_id\", \n",
    "    F.concat(F.lit(\"pvo_patrimoine_voirie.pvoamenagementcyclable.\"), F.col(\"amenagement_id\"))\n",
    ")\n",
    "\n",
    "# --- ROBUST NATIVE SPARK COORDINATE PARSING (No UDF = No Broken Pipe) ---\n",
    "# 1. Clean string: Remove brackets [], spaces, and quotes\n",
    "# 2. Split by comma to get a flat array of numbers [lon1, lat1, lon2, lat2...]\n",
    "df_clean = df_raw_features.withColumn(\n",
    "    \"cleaned_coords\", \n",
    "    F.split(F.regexp_replace(F.col(\"coordiantes\"), r\"[\\[\\]\\s]\", \"\"), \",\")\n",
    ")\n",
    "\n",
    "# 3. Explode to rows to process list elements\n",
    "# posexplode gives: pos (index), val (number as string)\n",
    "df_exploded = df_clean.select(\n",
    "    \"amenagement_id\", \n",
    "    F.posexplode(F.col(\"cleaned_coords\")).alias(\"pos\", \"val\")\n",
    ")\n",
    "\n",
    "# 4. Filter empty and cast to Double\n",
    "df_exploded = df_exploded.filter(F.length(F.col(\"val\")) > 0).withColumn(\"val\", F.col(\"val\").cast(\"double\"))\n",
    "\n",
    "# 5. GroupBy to calculate Main Centroid\n",
    "# Even Index (0, 2, 4...) = Longitude\n",
    "# Odd Index (1, 3, 5...) = Latitude\n",
    "df_coords = df_exploded.groupBy(\"amenagement_id\").agg(\n",
    "    F.avg(F.when(F.col(\"pos\") % 2 == 1, F.col(\"val\"))).alias(\"centroid_lat\"),\n",
    "    F.avg(F.when(F.col(\"pos\") % 2 == 0, F.col(\"val\"))).alias(\"centroid_lon\")\n",
    ")\n",
    "\n",
    "# 6. Join back to original features to recover metadata (nom, type, etc.)\n",
    "df_features = df_raw_features.join(df_coords, on=\"amenagement_id\", how=\"inner\") \\\n",
    "                             .filter(F.col(\"centroid_lat\").isNotNull())\n",
    "\n",
    "# B. Load Targets (Global Scores 2014-2025)\n",
    "path_scores = \"file:\" + os.path.abspath(\"amenagement_scoring_global_json_2\")\n",
    "df_scores = spark.read.json(path_scores).withColumnRenamed(\"score_global\", \"score\")\n",
    "\n",
    "# C. Join\n",
    "df_full = df_features.join(df_scores, on=\"amenagement_id\", how=\"inner\")\n",
    "\n",
    "print(f\"Dataset Size (Global): {df_full.count()} rows\")\n",
    "df_full.select(\"amenagement_id\", \"nom\", \"centroid_lat\", \"centroid_lon\", \"score\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prep_target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Tier Threshold (Top 10% Global): Score >= 0.7738\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|  406|\n",
      "|  1.0|   50|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) PREPARE TARGET (Top 10%)\n",
    "# =========================\n",
    "\n",
    "# Calculate Threshold on Global Scores\n",
    "quantile_90 = df_full.stat.approxQuantile(\"score\", [0.9], 0.01)[0]\n",
    "print(f\"Top Tier Threshold (Top 10% Global): Score >= {quantile_90:.4f}\")\n",
    "\n",
    "# Create Binary Label - Handle NULLs robustly (if score is NULL, label is 0.0)\n",
    "df_dataset = df_full.withColumn(\n",
    "    \"label\", \n",
    "    F.when(F.col(\"score\") >= quantile_90, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "df_dataset.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 17:27:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete.\n",
      "Model Performance (AUC): 0.9467\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) TRAIN RANDOM FOREST PIPELINE\n",
    "# =========================\n",
    "\n",
    "# Handle Categorical Features\n",
    "indexer_type = StringIndexer(inputCol=\"typeamenagement\", outputCol=\"type_idx\", handleInvalid=\"keep\")\n",
    "indexer_reseau = StringIndexer(inputCol=\"reseau\", outputCol=\"reseau_idx\", handleInvalid=\"keep\")\n",
    "encoder = OneHotEncoder(inputCols=[\"type_idx\", \"reseau_idx\"], outputCols=[\"type_vec\", \"reseau_vec\"])\n",
    "\n",
    "# Assemble Features (Geo is Key!)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"centroid_lat\", \"centroid_lon\", \"type_vec\", \"reseau_vec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, maxDepth=10)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[indexer_type, indexer_reseau, encoder, assembler, rf])\n",
    "\n",
    "# Train/Test Split\n",
    "train_data, test_data = df_dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Training Model...\")\n",
    "model = pipeline.fit(train_data)\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Model Performance (AUC): {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grid_simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using simulation features: Type='Piste Cyclable', Reseau='Réseau structurant et super structurant'\n",
      "Selecting Top 50 candidates...\n",
      "+------------------+------------------+-------------------+\n",
      "|      centroid_lat|      centroid_lon|       prob_success|\n",
      "+------------------+------------------+-------------------+\n",
      "|45.755102040816325| 4.839795918367347|  0.720142494549613|\n",
      "| 45.76122448979592| 4.839795918367347| 0.6062145753296765|\n",
      "|45.755102040816325| 4.843877551020408| 0.5981640066632304|\n",
      "| 45.75816326530612| 4.839795918367347| 0.5727544323939976|\n",
      "| 45.76428571428572| 4.839795918367347| 0.5550570661721673|\n",
      "|45.755102040816325|  4.84795918367347| 0.5398954227188242|\n",
      "| 45.76734693877551| 4.839795918367347|0.47520390881350744|\n",
      "|45.755102040816325|4.8520408163265305| 0.4571453235554165|\n",
      "|45.755102040816325| 4.856122448979592| 0.4571453235554165|\n",
      "| 45.77040816326531| 4.839795918367347| 0.4456252432126205|\n",
      "+------------------+------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) GRID SIMULATION (The Treasure Map)\n",
    "# =========================\n",
    "\n",
    "# Get best features from training data (Mode) to ensure valid inputs\n",
    "# We pick the most frequent type/network to simulate a \"standard\" good infrastructure\n",
    "common_type = df_dataset.groupBy(\"typeamenagement\").count().orderBy(F.desc(\"count\")).first()[\"typeamenagement\"]\n",
    "common_reseau = df_dataset.groupBy(\"reseau\").count().orderBy(F.desc(\"count\")).first()[\"reseau\"]\n",
    "print(f\"Using simulation features: Type='{common_type}', Reseau='{common_reseau}'\")\n",
    "\n",
    "# Define Bounding Box (Lyon approx)\n",
    "lat_min, lat_max = 45.70, 45.85\n",
    "lon_min, lon_max = 4.75, 4.95\n",
    "\n",
    "# Generate Grid Points (approx every 200m)\n",
    "lat_steps = np.linspace(lat_min, lat_max, 50)\n",
    "lon_steps = np.linspace(lon_min, lon_max, 50)\n",
    "\n",
    "grid_data = []\n",
    "for lat in lat_steps:\n",
    "    for lon in lon_steps:\n",
    "        # Simulate a infrastructure using the common valid types\n",
    "        grid_data.append((float(lat), float(lon), common_type, common_reseau))\n",
    "\n",
    "# Create DataFrame\n",
    "df_grid = spark.createDataFrame(grid_data, [\"centroid_lat\", \"centroid_lon\", \"typeamenagement\", \"reseau\"])\n",
    "\n",
    "# Predict Probability\n",
    "grid_predictions = model.transform(df_grid)\n",
    "\n",
    "# --- ROBUST NATIVE PROBABILITY EXTRACTION (No UDF = No Broken Pipe) ---\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# 'probability' is a DenseVector. vector_to_array converts it to [prob_class0, prob_class1]\n",
    "# We select index 1 (Success)\n",
    "df_heatmap = grid_predictions.withColumn(\"prob_array\", vector_to_array(\"probability\")) \\\n",
    "                             .withColumn(\"prob_success\", F.col(\"prob_array\")[1])\n",
    "\n",
    "# STRATEGY: TOP 50 RANKING (No strict threshold)\n",
    "# We want the 50 best locations, whatever their absolute score is.\n",
    "top_candidates = df_heatmap.orderBy(F.desc(\"prob_success\")).limit(50)\n",
    "\n",
    "print(f\"Selecting Top 50 candidates...\")\n",
    "top_candidates.select(\"centroid_lat\", \"centroid_lon\", \"prob_success\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "export_output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prediction Map exported to: /Users/youness/Desktop/datathon_velomenaj/predictions_heatmap_lyon_2.json\n",
      "Rows written: 50\n",
      "   centroid_lat  centroid_lon  prob_success    recommendation\n",
      "0     45.755102      4.839796      0.720142  Top-50 Potential\n",
      "1     45.761224      4.839796      0.606215  Top-50 Potential\n",
      "2     45.755102      4.843878      0.598164  Top-50 Potential\n",
      "3     45.758163      4.839796      0.572754  Top-50 Potential\n",
      "4     45.764286      4.839796      0.555057  Top-50 Potential\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) EXPORT OUTPUT (JSON)\n",
    "# =========================\n",
    "output_file = \"predictions_heatmap_lyon_2.json\"\n",
    "\n",
    "# Collect to Driver (small data) and write JSON\n",
    "pdf_candidates = top_candidates.select(\"centroid_lat\", \"centroid_lon\", \"prob_success\").toPandas()\n",
    "pdf_candidates[\"recommendation\"] = \"Top-50 Potential\"\n",
    "\n",
    "pdf_candidates.to_json(output_file, orient='records', indent=4)\n",
    "print(f\"✅ Prediction Map exported to: {os.path.abspath(output_file)}\")\n",
    "print(f\"Rows written: {len(pdf_candidates)}\")\n",
    "print(pdf_candidates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c4bae-3958-40a0-a3e6-e2b47ed8fb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
